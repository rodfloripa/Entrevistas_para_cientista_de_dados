Q1- Whats the trade-off between bias and variance?

Essentially, if you make the model more complex and add more variables, 
youll lose bias error but gain some variance error in order to get the optimally
 reduced amount of error, youll have to tradeoff bias and variance. 
High bias error causes underfitting and high variance model overfitting

Q2- What is the difference between supervised and unsupervised machine learning?

Supervised learning requires training labeled data. For example,
 in order to do classification (a supervised learning task),
 youll need to first label the data youll use to train the model to classify
 data into your labeled groups. Unsupervised learning, in contrast, does not
 require labeling data explicitly.

Q3- How is KNN different from k-means clustering?

The critical difference here is that KNN needs labeled points and is thus
 supervised learning, while k-means doesnt  and is thus unsupervised learning.

Q4- Explain how a ROC curve works.

The ROC curve is a graphical representation of the contrast between true
 positive rates and the false positive rate at various thresholds. Its 
often used as a proxy for the trade-off between the sensitivity of the model
 (true positives) vs the fall-out or the probability it will trigger a false 
alarm (false positives).

Q5- Define precision and recall.

Recall is also known as the true positive rate: the amount of positives
 your model claims compared to the actual number of positives there are 
throughout the data. Precision is also known as the positive predictive value,
 and it is a measure of the amount of accurate positives your model claims 
compared to the number of positives it actually claims.

Q6- What is Bayes Theorem? How is it useful in a machine learning context?

Mathematically, its expressed as:
 P(A/B) =  P(B/A)*P(A)/P(B)

Q7- Why is Naive Bayes naive?

This is a simplification of Bayes theorem, implies all of the features
 are mutually independent,a condition probably never met in real life.
P(A|B,C,D,...) = P(A)

Q8- Explain the difference between L1 and L2 regularization.

L2 regularization tends to spread error among all the terms, while L1 is more
 binary/sparse, with many variables either being assigned a 1 or 0 in weighting.
 L1 corresponds to setting a Laplacean prior on the terms, while L2 corresponds
 to a Gaussian prior.

Q10- Whats the difference between Type I and Type II error?

Type I error is a false positive, while Type II error is a false negative.
 Briefly stated, Type I error means claiming something has happened when it hasnt, 
while Type II error means that you claim nothing is happening when in fact something is.

Q12- Whats the difference between probability and likelihood?

Probability is the chance of success(discrete), likelihood is the conditional
probability(continuous), the chance of sucess given variables.

Q13- What is deep learning, and how does it contrast with other machine learning algorithms?

Deep learning is a subset of machine learning that is concerned with neural networks:
how to use backpropagation and certain principles from neuroscience to more accurately
model large sets of unlabelled or semi-structured data. In that sense, deep learning 
represents an unsupervised learning algorithm that learns representations of data
 through the use of neural nets.

Q14- Whats the difference between a generative and discriminative model?

A generative model will learn categories of data while a discriminative model
 will simply learn the distinction between different categories of data. 
Discriminative models will generally outperform generative models on classification tasks.

Q15- Which cross-validation technique would you use on a time series dataset?

Split data into n parts, train on n= 1:(n-1) and test on nth part

fold 1 : training [1], test [2]
fold 2 : training [1 2], test [3]
fold 3 : training [1 2 3], test [4]
fold 4 : training [1 2 3 4], test [5]
fold 5 : training [1 2 3 4 5], test [6]

Q16- How is a decision tree pruned(podada)?

Pruning is what happens in decision trees when branches that have weak predictive
 power are removed in order to reduce the complexity of the model and increase 
the predictive accuracy of a decision tree model.

Q17- Which one is more important to you model accuracy, or model performance?

Accuracy= proportion of correctly identified predictions(positives and negatives)
There are models with higher accuracy that can perform worse in predictive power 
Performance is more important

Q18- Whats the F1 score? How would you use it?

The F1 score is a measure of a models performance. It is a weighted average
 of the precision and recall of a model(only positives), with results tending to 1 being the best,
 and those tending to 0 being the worst.You would use it in classification tests
 where true negatives dont matter much.

Q19- Describe some performance measures

F1Score,Mean absolute error,mean squared error,area under curve,
confusion matrix,logarithmic loss

Q19- How would you handle an imbalanced dataset?

Collect more data,change the metric to Roc,resample,create synthetic
samples(smote),try other algorithms,try penalized(for minor class) svm or lda,
try anomaly detection and change detection

Q21- Name an example where ensemble techniques might be useful.

They typically reduce overfitting in models and make the model
 more robust (unlikely to be influenced by small changes in the training data). 
You could list some examples of ensemble methods, from bagging to boosting

Q22- How do you avoid overfitting?

There are three main methods to avoid overfitting:
1- Keep the model simpler: reduce variance by taking into account fewer
 variables and parameters, thereby removing some of the noise in the training data.

2- Use cross-validation techniques such as k-folds cross-validation.

3- Use regularization techniques such as LASSO that penalize certain model
 parameters if theyre likely to cause overfitting.

Q23- What evaluation approaches would you work to gauge the effectiveness
 of a machine learning model?

Use cross-validation techniques. You should then implement a choice selection
 of performance metrics: here is a fairly comprehensive list. You could use
 measures such as the F1 score, the accuracy, and the confusion matrix. 

Q25- Whats the kernel trick and how is it useful?

Is a way of transforming a space with non linearly separable classes in  a bigger space where the classes are linearly separable
The scalable product of the bigger space are used to train the model,making it computationally efficient.

Q26- How do you handle missing or corrupted data in a dataset?

You could find missing/corrupted data in a dataset and either drop those rows(ignore)
 or columns, or decide to replace them with another value.

Q29- What are some differences between a linked list and an array?

An array is an ordered collection of objects. A linked list is a series
 of objects with pointers that direct how to process them sequentially. 
An array assumes that every element has the same size, unlike the linked list.
 A linked list can more easily grow organically: an array has to be pre-defined
 or re-defined for organic growth. Shuffling a linked list involves changing which
 points direct where  meanwhile, shuffling an array is more complex and takes more memory.

Q30- Describe a hash table

A hash table is a data structure that produces an associative array. 
A key is mapped to certain values through the use of a hash function.
 They are often used for tasks such as database indexing.

Q31- Which data visualization libraries do you use?
 What are your thoughts on the best data visualization tools?

Matplotlib and sometimes Seaborn
The best tool is the one that I master and produce plots very fast.
For me Matplotlib is a very simple tool, I really know its syntax by heart.

Q32- How would you implement a recommendation system for our companys users?

Each product should have a set of words associated with it,and when a product
is shown, the products that share the same words are recommended.

Q33- How can we use your machine learning skills to generate revenue?

Monitor social networks in order make a sentimental analysis about
the products.Understand the profile of the consumer using cluster
analysis, to target market campaigns. Use machine learning to understand what kind
of features affect campaign's effectivity.Create a recommendation system
in order to make effective marketing. Predict sales, in order to optimize
operation,employees needed,products.

Q35- What are the last machine learning papers youve read?

A paper about a new regression algorithm that is differentiable.
The model learned can be used for example to predict house prices
based on a lot of aspects and produce a contour plot of the variation
of prices along geographical paths.This contour plot would be shown
over the map of the city.

Q37- What are your favorite use cases of machine learning models?

Reccomendation systems, sales prediction, customer segmentation.

Q38-You are given a train data set having 1000 columns and 1 million rows.
 The data set is based on a classification problem. Your manager has asked
 you to reduce the dimension of this data so that model computation time
 can be reduced. Your machine has memory constraints. What would you do?
 (You are free to make practical assumptions.)

PCA on numeric variables,Sample some columns,use online learning, 
linear model with Stochastic Gradient Descent

Q39. Is rotation needed in PCA? If yes, Why? What will happen if you dont rotate the components?

Depends on the number of columns.
Rotation maximizes the difference  between variance captured by the component. 
This makes the components easier to interpret.If we dont rotate the components, 
the effect of PCA will diminish and well have to select more number of components
 to explain variance in the data set.
This is called PCA+varimax rotation,the components will no longer have orthogonality!!!

Q40. You are given a data set. The data set has missing values which spread along 1 
standard deviation from the median. What percentage of data would remain unaffected? Why?

Lets assume its a normal distribution. We know, in a normal distribution, ~68% of the
 data lies in 1 standard deviation from mean (or mode, median), which leaves ~32% of
 the data unaffected.

Q41. What is likelihood?

Likelihood is the probability of classifying a given observation as 1 in presence
 of some other variable.
Marginal likelihood is the probability that the variable is present in all classes.

Q42. You are working on a time series data set. Your manager has asked you to build 
a high accuracy model. You start with the decision tree algorithm, since you know
 it works fairly well on all kinds of data. Later, you tried a time series regression
 model and got higher accuracy than decision tree model. Can this happen? Why?

Yes.Because decision trees work better with non linear data, while a regression
works better in linear data.In this example, the data set is linear.

Q43. You are assigned a new project which involves helping a food delivery 
company save more money. The problem is, companys delivery team arent able
to deliver food on time. As a result, their customers get unhappy. And, to keep
them happy, they end up delivering food for free. Which machine learning algorithm can save them?

This is a route optimization problem. A machine learning problem consists of three things:

There exist a pattern.
You cannot solve it mathematically (even by writing exponential equations).
You have data on it.

Q44. How can you lower the variance of an algorithm?

Using regularization to penalize complex models, use n top features from
variance importance chart.

Q45. You came to know that your model is suffering from low bias and high
 variance. Which algorithm should you use to tackle it?

Bagging or boosting

Q46. You are given a data set. The data set contains many variables,
 some of which are highly correlated and you know about it. Your manager
 has asked you to run PCA. Would you remove correlated variables first? Why?

Discarding correlated variables have a substantial effect on PCA because,
in presence of correlated variables, the variance explained by a particular
component gets inflated.

Q47. After spending several hours, you are now anxious to build a high accuracy model.
 As a result, you build 5 gradient boosting models, thinking a boosting algorithm would do the magic.
 Unfortunately, neither of models could perform better than benchmark score. Finally,
 you decided to combine those models. Though, ensembled models are known to return high 
accuracy, but you are unfortunate. Where did you miss?

Ensemble learners are based on the idea of combining weak learners to create strong
 learners. But, these learners provide superior result when the combined models are
 uncorrelated. Since, we have used 5 GBM models and got no accuracy improvement, 
suggests that the models are correlated. The problem with correlated models is, all
 the models provide same information.

Q48. Explain the Knn algorithm

Knn uses k surrounding neighbours to classify(predict) an observation.

Q49. How is True Positive Rate and Recall related? Write the equation.

TPR = TP/(TP+FN) =  Recall 

Q50. List the metrics of the confusion matrix

Accuracy: proportion of correctly identified predictions
Precision: proportion of positive correctly identified predictions
Negative Predictive Value: proportion of negative correctly identified predictions
Sensitivity or Recall : the proportion of actual positive cases which are correctly identified
Specificity : the proportion of actual negative cases which are correctly identified

Q51. You have built a multiple regression model. Your model R² isnt as good as
 you wanted. For improvement, your remove the intercept term, your model R² becomes
 0.8 from 0.3. Is it possible?

Yes.Absence of intercept results in smaller R².

Q52. After analyzing the model, your manager has informed that your regression model
 is suffering from multicollinearity. How would you check if hes true? Without losing
 any information, can you still build a better model?

We can create a correlation matrix to identify & remove variables having correlation
 above 75% (deciding a threshold is subjective)
In order to retain those variables, we can use penalized regression models like ridge.

Q53. When is Ridge regression favorable over Lasso regression?

L1= Lasso, L2= Ridge
In presence of few variables with medium / large sized effect, use lasso regression.
 In presence of many variables with small / medium sized effect, use ridge regression.
Conceptually, we can say, lasso regression (L1) does both variable selection and parameter
 shrinkage, whereas Ridge regression only does parameter shrinkage and end up including all
 the coefficients in the model. In presence of correlated variables, ridge regression might
 be the preferred choice

Q54. Rise in global average temperature led to decrease in number of pirates around
 the world. Does that mean that decrease in number of pirates caused the climate change?

Therefore, there might be a correlation between global average temperature and number
 of pirates, but based on this information we cant say that pirated died because of
 rise in global average temperature.

Q55. While working on a data set, how do you select important variables? Explain your methods.

Following are the methods of variable selection you can use:

-Remove the correlated variables prior to selecting important variables
-Use linear regression and select variables based on p values
-Use Forward Selection, Backward Selection, Stepwise Selection
-Use Random Forest, Xgboost and plot variable importance chart
-Use Lasso Regression
-Measure information gain for the available set of features and select top n features accordingly.

Q56. What is the difference between covariance and correlation?

Correlation is the standardized form of covariance.
Covariances are difficult to compare. For example: if we calculate the
 covariances of salary ($) and age (years), well get different covariances
 which cant be compared because of having unequal scales. To combat such 
situation, we calculate correlation to get a value between -1 and 1, irrespective
 of their respective scale.

Q57. Is it possible capture the correlation between continuous and categorical variable? If yes, how?

Yes, we can use ANCOVA (analysis of covariance) technique to capture association between
 continuous and categorical variables.

Q58. Both being tree based algorithm, how is random forest different from Gradient
 boosting algorithm (GBM)?

The fundamental difference is, random forest uses bagging technique to make
 predictions. GBM uses boosting techniques to make predictions.
In bagging technique, a data set is divided into n samples using randomized sampling. 
Then, using a single learning algorithm a model is build on all samples. Later, the
 resultant predictions are combined using voting or averaging. Bagging is done is parallel.
 In boosting, after the first round of predictions, the algorithm weighs misclassified
 predictions higher, such that they can be corrected in the succeeding round. This
 sequential process of giving higher weights to misclassified predictions continue 
until a stopping criterion is reached.
Random forest improves model accuracy by reducing variance (mainly). 
The trees grown are uncorrelated to maximize the decrease in variance. 
On the other hand, GBM improves accuracy my reducing both bias and variance error in a model.

Q59. Running a binary classification tree algorithm is the easy part. Do you know
 how does a tree splitting takes place i.e. how does the tree decide which variable to
 split at the root node and succeeding nodes?

A classification trees makes decision based on Gini Index and Node Entropy. In 
simple words, the tree algorithm finds the best possible feature which can divide
the data set into purest possible children nodes.
Gini: (p^2+q^2)
Entropy: -plog(p)-qlog(q)
p and q: probability of success and failure
Lower entropy is desirable

Q60. Youve built a random forest model with 10000 trees. You got delighted after
 getting training error as 0.00. But, the validation error is 34.23. What is going on? 
Havent you trained your model perfectly?

The model has overfitted,because the number of trees is higher than necessary
Hence, to avoid these situation, we should tune number of trees using cross validation.

Q61. Youve got a dataset to work having p (no. of variable) > n (no. of observation).
 Why is OLS as bad option to work with? Which techniques would be best to use? Why?

The variances become infinite(matrix is not full rank,not invertible), so OLS cannot be used at all.
Use Lasso or Ridge, the last is better with higher variance

Q62. We know that one hot encoding increases the dimensionality of a data set. But,
 label encoding doesnt. How ?

One hot creates a new variable for each level present in categorical variables.
In label encoding, the levels of categorical variables get encoded as 0 and 1, 
so no new variable are created

Q63. You are given a dataset consisting of variables having more than 30% missing values:
Lets say, out of 50 variables, 8 variables have missing values higher than 30%. How 
will you deal with them?

Assign a unique category to missing values, who knows the missing values might decipher some trend
We can't remove them blatantly.
Or, we can sensibly check their distribution with the target variable, and if found any pattern
 well keep those missing values and assign them a new category while removing others.

Q64. People who bought this, also bought recommendations seen on amazon is a result of which algorithm?

Collaborative Filtering.The algorithm considers User Behavior for recommending items.
They exploit behavior of other users and items in terms of transaction history, 
ratings, selection and purchase information. Other users behaviour and preferences over
 the items are used to recommend items to the new users.

Q65. What do you understand by Type I vs Type II error in the context of hypothesis?

Type I error is committed when the null hypothesis is true and we reject it
Type II error is committed when the null hypothesis is false and we accept it

Q66. You are working on a classification problem. For validation purposes, youve randomly 
sampled the training data set into train and validation. You are confident that your model(which is not complex)
 will work incredibly well on unseen data since your validation accuracy is high. However,
 you get shocked after getting poor test accuracy. What went wrong?

The model is not complex,so overfitting is not occurring. 
In case of classification problem, we should always use stratified sampling instead of
 random sampling. A random sampling doesnt takes into consideration the proportion of
 target classes. On the contrary, stratified sampling helps to maintain the distribution 
of target variable in the resultant distributed samples also.

Q67. You have been asked to evaluate a regression model based on R², adjusted R² and
tolerance. What will be your criteria?

Tolerance (1 / VIF) is used as an indicator of multicollinearity
We will consider adjusted R² as opposed to R² to evaluate model fit because R² increases 
irrespective of improvement in prediction accuracy as we add more variables.

Q68. In k-means or kNN, we use euclidean distance to calculate the distance between nearest
 neighbors. Why not manhattan distance ?

We dont use manhattan distance because it calculates distance horizontally or vertically 
only. It has dimension restrictions. On the other hand, euclidean metric can be used in any
 space to calculate distance. 

Q69. Explain machine learning to me like a 5 year old child.

 Its just like how babies learn to walk. Every time they fall down, they learn (unconsciously)
 & realize that their legs should be straight and not in a bend position. The next time they
 fall down, they feel pain.To succeed, they even seek support from the door or wall or anything
 near them, which helps them stand firm. 

Q70. I know that a linear regression model is generally evaluated using Adjusted R² or F value.
 How would you evaluate a logistic regression model?

We can use AUC-ROC curve along with confusion matrix to determine its performance.
Also, the analogous metric of adjusted R² in logistic regression is AIC. AIC is 
the measure of fit which penalizes model for the number of model coefficients.
 Therefore, we always prefer model with minimum AIC value.
Null Deviance indicates the response predicted by a model with nothing but an intercept.
 Lower the value, better the model. Residual deviance indicates the response predicted
 by a model on adding independent variables. Lower the value, better the model.

Q71. Considering the long list of machine learning algorithm, given a data set, how do
 you decide which one to use?

If you are given a data set which is exhibits linearity, then linear regression would be
 the best algorithm to use. If you given to work on images, audios, then neural network
 would help you to build a robust model.
If the data comprises of non linear interactions, then a boosting or bagging algorithm
 should be the choice. If the business requirement is to build a model which can be deployed,
 then well use regression or a decision tree model (easy to interpret and explain) instead
 of black box algorithms like SVM, GBM

Q72. Do you suggest that treating a categorical variable as continuous variable would result
 in a better predictive model?

For better predictions, categorical variable can be considered as a continuous variable
only when the variable is ordinal in nature.

Q73. When does regularization becomes necessary in Machine Learning?

Regularization becomes necessary when the model begins to ovefit / underfit
This technique introduces a cost term for bringing in more features with the objective function

Q74. OLS is to linear regression. Maximum likelihood is to logistic regression. Explain the statement.

Ordinary least square(OLS) is a method used in linear regression which approximates
 the parameters resulting in minimum distance between actual and predicted values. 
Maximum Likelihood helps in choosing the the values of parameters which maximizes the
 likelihood between the pdf of observed data and predicted.

Q78. Describe dimensional reductions that you know and their characteristics

PCA- used for numerical data,non supervised
LDA- used for numerical data,supervised and assumes gaussian distribution to split classes
t-sne- non linear reduction, pca is needed before

Q79. What should be the frequency distribution of features of a model when
 linear regression will be used?

Normal distribution,if it is not normal a box-cox transformation maybe could help

Q80. What are the assumptions of log regression?

Homocedasticity is not needed,does not require a linear relationship between the
dependent and independent variables,error terms (residuals) can't be normally distributed

Q81. How do you calculate the sample size for a log regression?

feature_number*10/prob of least frequent class
For example: you have 3 covariates to include in the model and the proportion
of positive cases in the population is 0.20 (20%). The minimum number of cases
required is N = 10 x 3 / 0.20 = 150

Q82.Which kinds of clustering exist?

Hard clustering: each point belongs to a cluster
Soft clustering: each point can belong to more than one cluster

Q83. Wich kinds of clustering algos exist?

Centroid,Distribution,Hierarchical,Density

Q84. How does k-means work?

Find points randomnly attributed to clusters and calculate the centroid.
Points next to clusters will be attributed to the cluster.When you have two
consecutive iterations whithout any change in cluster the algo ends.

Q85. How does hierarchical clustering work?

At the begining, set all the points to a cluster and start splitting in more clusters.
Or,start with each point belonging to a cluster and group the clusters.

Q86. Which kinds of distance formulas are used in hierarchical clustering?

Manhattan,Euclidean,Mahalanobis,Maximum

Q87. In hierarchical clustering how do you choose the optimal clustering number?

Look in the hierarchical graph and choose the biggest vertical distance without
cluster intersection.

Q88. What are the characteristics of k-means?

O(n),requires previous knowledge of k,can give different results,works good
with spherical data.

Q89. What is the main characteristic of Hierarchical Clustering?

O(n²)

Q90. What are the applications of clustering?

Anomaly detection,recomendation systems,client segmentation,social network analysis,
image segmentation,grouping of search results.

Q91. What is the relationship of clustering and regression?

You can use clustering followed by regression: number of cluster,size of cluster,
centroid.You can use one regression for each cluster.

Q92. What can give a different result in Hierarchical clustering?

Change in distance function,number of points,variables

Q93. What can change the speed of convergence in k-means?

Bad or good initialization

Q94. How can you optimize k-means?

Optimal number of clusters,adjusting the number of iterative steps,
run with different initialization centroids.

Q95. Which reduction algos need data scaling?

LDA and PCA

Q96. Which ML algos don't need dada scaling?

Tree based algos.

Q.97 In which kind of applications we should use z-standarized and min-max transformations?

z: cluster analysis and pca
min-max: image processing(0-255) and neural networks(0-1)

Q98. Which transformation you should use before a linear regression?

Box-cox

Q99. What are the assumptions of linear regression?

Homocedasticity between residuals an variables,gaussian distribution of residuals, linear relationship between X and Y,independent observations

Q100. What do you need to calculate sample size?

Size of population,confidence, error margin between sample mean and population mean,variance(default=0.5)


